{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d500cca",
   "metadata": {},
   "source": [
    "# MULTI-DOCUMENT TEXT SUMMARIZATION\n",
    "\n",
    "The goal of text summarizing is to see if we can come up with a method that employs natural language processing to do so. This method will not only save time in comprehending a text, but it will also allow someone to read multiple texts in a short period of time, saving time in the long term.\n",
    "\n",
    "\n",
    "### TYPES OF TEXT SUMMARIZATION\n",
    "\n",
    "There are two types of Text Summarization: \n",
    "1. Extractive Type \n",
    "2. Abstractive Type \n",
    "\n",
    "Extractive summarization takes the original text and extracts information that is identical to it. In other words, rather than providing a unique summary based on the full content, it will rate each sentence in the document against all others, based on how well each line explains. \n",
    "\n",
    "Abstractive seeks to construct a one-of-a-kind summary by learning the most significant points from the original text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79dbeb",
   "metadata": {},
   "source": [
    "# EXTRACTIVE SUMMARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "59939d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the required libraries\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from heapq import nlargest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d8931cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input from dataset\n",
    "\n",
    "src_file = \"./dataset/multi_news/test.truncate.fix.pun.src.txt\"\n",
    "with open(src_file, \"r+\", encoding=\"utf8\") as f:\n",
    "    s = f.readlines()\n",
    "\n",
    "input = s[:20]\n",
    "\n",
    "out_file = \"./dataset/multi_news/test.txt.tgt.tokenized.fixed.cleaned.final.truncated.txt\"\n",
    "with open(out_file, \"r+\", encoding=\"utf8\") as f:\n",
    "    s = f.readlines()\n",
    "\n",
    "output = s[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1f865574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing text (input) data\n",
    "\n",
    "for text in input:\n",
    "    text=text.replace(\"x91\",\"'\")\n",
    "    text=text.replace(\"x92\",\"'\")\n",
    "    \n",
    "for text in output:\n",
    "    text=text.replace(\"x91\",\"'\")\n",
    "    text=text.replace(\"x92\",\"'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29b68e",
   "metadata": {},
   "source": [
    "## FREQUENCY-BASED APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8c1103cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, n):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word.lower() for word in word_tokenize(text) if word.lower() not in stop_words and word.isalnum()]\n",
    "\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    sentence_scores = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_words = [word.lower() for word in word_tokenize(sentence) if word.lower() not in stop_words and word.isalnum()]\n",
    "        sentence_score = sum([word_freq[word] for word in sentence_words])\n",
    "        if len(sentence_words) < 20:\n",
    "            sentence_scores[sentence] = sentence_score\n",
    "\n",
    "        # Select the top n sentences with the highest scores\n",
    "        summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:n]\n",
    "        summary = ' '.join(summary_sentences)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1bda8",
   "metadata": {},
   "source": [
    "## FREQUENCY BASED USING TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9af9af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_tfidf(text, n):\n",
    "    # Tokenize the text into individual sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Create the TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Compute the cosine similarity between each sentence and the document\n",
    "    sentence_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n",
    "\n",
    "    # Select the top n sentences with the highest scores\n",
    "    summary_sentences = nlargest(n, range(len(sentence_scores)), key=sentence_scores.__getitem__)\n",
    "\n",
    "    summary_tfidf = ' '.join([sentences[i] for i in sorted(summary_sentences)])\n",
    "\n",
    "    return summary_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "046091f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "summary_sentences = []\n",
    "formatted_summary = []\n",
    "\n",
    "for i in range(0, 20):\n",
    "    s = generate_summary(input[i], 9)\n",
    "    f = s.split('. ')\n",
    "    e = '.\\n'.join(f)\n",
    "    summary.append(s)\n",
    "    summary_sentences.append(f)\n",
    "    formatted_summary.append(e)\n",
    "\n",
    "\n",
    "summary_tfidf = []\n",
    "summary_sentences_tfidf = []\n",
    "formatted_summary_tfidf = []\n",
    "\n",
    "for i in range(0, 20):\n",
    "    s = generate_summary_tfidf(input[i], 9)\n",
    "    f = s.split('. ')\n",
    "    e = '.\\n'.join(f)\n",
    "    summary_tfidf.append(s)\n",
    "    summary_sentences_tfidf.append(f)\n",
    "    formatted_summary_tfidf.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd7255",
   "metadata": {},
   "source": [
    "## ROGUE SCORES FOR THESE TWO METHODS OF SUMMARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8b0c6d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in d:\\softwares\\anaconda\\lib\\site-packages (from rouge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n",
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge_score(text, summary):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(summary, text)\n",
    "    rouge_1_recall = scores[0]['rouge-1']['f']\n",
    "    rouge_2_recall = scores[0]['rouge-2']['f']\n",
    "    rouge_l_recall = scores[0]['rouge-l']['f']\n",
    "    return rouge_1_recall, rouge_2_recall, rouge_l_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337658f",
   "metadata": {},
   "source": [
    "### For Frequency based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e68acb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROGUE Scores for Frequency based approach\n",
      "ROUGE-1: 0.2766996648238301\n",
      "ROUGE-2: 0.08311613973407764\n",
      "ROUGE-L: 0.24519889172116605\n"
     ]
    }
   ],
   "source": [
    "rouge1, rouge2, rougel = 0,0,0\n",
    "\n",
    "for i in range(0,20):\n",
    "    rouge_1_recall, rouge_2_recall, rouge_l_recall = calculate_rouge_score(output[i], formatted_summary[i])\n",
    "    rouge1 += rouge_1_recall\n",
    "    rouge2 += rouge_2_recall\n",
    "    rougel += rouge_l_recall\n",
    "\n",
    "print(\"ROGUE Scores for Frequency based approach\")\n",
    "print(\"ROUGE-1:\", rouge1/20)\n",
    "print(\"ROUGE-2:\", rouge2/20)\n",
    "print(\"ROUGE-L:\", rougel/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b35f8f",
   "metadata": {},
   "source": [
    "### For Frequncy based using Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8bf56ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROGUE Scores for Frequency based Tf-Idf approach\n",
      "ROUGE-1: 0.29930268602339466\n",
      "ROUGE-2: 0.10104034882506026\n",
      "ROUGE-L: 0.26855892342746535\n"
     ]
    }
   ],
   "source": [
    "rouge1, rouge2, rougel = 0,0,0\n",
    "\n",
    "for i in range(0,20):\n",
    "    rouge_1_recall, rouge_2_recall, rouge_l_recall = calculate_rouge_score(output[i], formatted_summary_tfidf[i])\n",
    "    rouge1 += rouge_1_recall\n",
    "    rouge2 += rouge_2_recall\n",
    "    rougel += rouge_l_recall\n",
    "\n",
    "print(\"ROGUE Scores for Tf-Idf approach\")\n",
    "print(\"ROUGE-1:\", rouge1/20)\n",
    "print(\"ROUGE-2:\", rouge2/20)\n",
    "print(\"ROUGE-L:\", rougel/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf487529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
